{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tSS7wCtP31aC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'chàng trai 9x quảng_trị khởi_nghiệp từ nấm sò'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from underthesea import word_tokenize\n",
        "import string\n",
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub('<.*?>', '', text).strip()\n",
        "    text = re.sub('(\\s)+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def remove_numbers(text_in):\n",
        "  for ele in text_in.split():\n",
        "    if ele.isdigit():\n",
        "        text_in = text_in.replace(ele, \"@\")\n",
        "  for character in text_in:\n",
        "    if character.isdigit():\n",
        "        text_in = text_in.replace(character, \"@\")\n",
        "  return text_in\n",
        "\n",
        "\n",
        "def remove_special_characters(text):\n",
        "  chars = re.escape(string.punctuation)\n",
        "  return re.sub(r'['+chars+']', '', text)\n",
        "\n",
        "\n",
        "def word_segment(sent):\n",
        "  # sent = \" \".join(word_tokenize(sent.replace(\"\\n\", \" \").lower())[0])\n",
        "  sent = word_tokenize(sent, format='text')\n",
        "  return sent.lower()\n",
        "\n",
        "\n",
        "def preprocess(text_in):\n",
        "    text = clean_text(text_in)\n",
        "    text = remove_special_characters(text)\n",
        "    # text = remove_numbers(text)\n",
        "    text = word_segment(text)\n",
        "    return text\n",
        "\n",
        "text = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
        "preprocess(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l7G5LTWC5aQk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trong này hơi tối nhỉ bật đèn trần lên\n",
            "trong này sáng quá tắt bớt đèn\n",
            "trong này hơi_lạnh nhỉ tắt máy_lạnh\n",
            "trong này hơi_lạnh nhỉ tắt quạt\n",
            "trong này hơi nóng nhỉ bật máy_lạnh\n",
            "trong này hơi nóng bật quạt\n",
            "không cần bật máy_lạnh đâu bật quạt được rồi\n",
            "bật điện phòng ngủ\n",
            "mở điện phòng ngủ\n",
            "bật điện phòng khách lầu 1\n",
            "bật điện phòng khách tầng 1\n",
            "bật điện phòng khách tầng 2\n",
            "bật điện phòng khách lầu trệt\n",
            "bật điện phòng khách lầu 3\n",
            "bật điện phòng khách lầu 4\n",
            "mở điện phòng khách lầu 1\n",
            "mở điện phòng khách lầu 2\n",
            "mở điện phòng khách lầu 3\n",
            "mở điện phòng khách lầu 4\n",
            "mở điện phòng khách lầu 5\n",
            "mở điện phòng khách tầng 1\n",
            "mở điện phòng khách tầng 2\n",
            "mở điện phòng khách tầng 3\n",
            "mở điện phòng khách tầng 4\n",
            "mở điện phòng khách tầng 5\n",
            "bật điện phòng_ngủ hiện_tại\n",
            "bật điện phòng ngủ lầu 1\n",
            "bật điện phòng_ngủ lầu 2\n",
            "bật điện phòng_ngủ lầu 3\n",
            "bật điện phòng ngủ lầu 4\n",
            "bật điện phòng_ngủ lầu 5\n",
            "bật điện phòng_ngủ ở hồ_chí_minh\n",
            "bật điện phòng_ngủ ở sài_gòn\n",
            "bật điện phòng_ngủ ở hà_nội\n",
            "bật điện phòng_ngủ ở huế\n",
            "bật điện phòng ngủ lầu 1 nhà của đức ở hồ_chí_minh\n",
            "bật điện phòng_ngủ tầng 1 nhà của khang ở huế\n",
            "bật điện phòng_ngủ tầng 1 ở sài_gòn\n",
            "bật điện phòng_ngủ tầng 1 ở hà_nội\n",
            "bật điện phòng_ngủ tầng 1 ở huế\n",
            "bật điện phòng_ngủ tầng 2\n",
            "bật điện phòng_ngủ tầng 2 ở hồ_chí_minh\n",
            "bật điện phòng_ngủ tầng 2 ở sài_gòn\n",
            "bật điện phòng_ngủ lầu 2 ở hà_nội\n",
            "bật điện phòng_ngủ lầu 2 ở huế\n",
            "bật điện phòng_ngủ lầu 3\n",
            "bật điện phòng_ngủ lầu 3 ở hồ_chí_minh\n",
            "bật điện phòng_ngủ lầu 3 ở sài_gòn\n",
            "bật điện phòng_ngủ lầu 3 ở hà_nội\n",
            "bật điện phòng_ngủ lầu 3 ở huế\n",
            "bật điện phòng ngủ lầu 4\n",
            "bật điện phòng ngủ lầu 4 ở hồ_chí_minh\n",
            "bật điện phòng ngủ lầu 4 ở sài_gòn\n",
            "bật điện phòng ngủ lầu 4 ở hà_nội\n",
            "bật điện phòng ngủ lầu 4 ở huế\n",
            "bật điện phòng_ngủ lầu 5\n",
            "bật điện phòng_ngủ lầu 5 ở hồ_chí_minh\n",
            "bật điện phòng_ngủ lầu 5 ở sài_gòn\n",
            "bật điện phòng_ngủ lầu 5 ở hà_nội\n",
            "bật điện phòng_ngủ lầu 5 ở huế\n",
            "bật điện phòng bếp\n",
            "bật điện phòng bếp hiện_tại\n",
            "bật điện phòng bếp lầu 1\n",
            "bật điện phòng bếp lầu 2\n",
            "bật điện phòng bếp lầu 3\n",
            "bật điện phòng bếp lầu 4\n",
            "bật điện phòng bếp lầu 5\n",
            "bật điện nhà_vệ_sinh\n",
            "bật điện nhà_vệ_sinh lầu 1\n",
            "bật điện nhà_vệ_sinh lầu 2\n",
            "bật điện nhà_vệ_sinh lầu 3\n",
            "bật điện nhà_vệ_sinh lầu 4\n",
            "bật điện nhà_vệ_sinh lầu 5\n",
            "bật điện lầu 1\n",
            "bật điện lầu 2\n",
            "bật điện lầu 3\n",
            "bật điện lầu 4\n",
            "bật điện lầu 5\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp\n",
            "bật giùm cái quạt_trần phòng khách lầu 1 nhà đức ở gò_vấp\n",
            "bật hết điện hành_lang lầu 3 nhà hà_nội\n",
            "bật các thiết_bị điện phòng khách\n",
            "bật cái tivi 2 lầu 1 và tắt cái tivi 1 lầu 2\n",
            "bật cái điều_hòa không_khí lầu 2 nhà hồ_chí_minh với\n",
            "bật bình nóng lạnh\n",
            "trời nay mát nên bật quạt được rồi\n",
            "mở rèm cửa tất_cả phòng ngủ\n",
            "mở hết camera trừ camera số 3 nhà số 1\n",
            "mở camera số 1 ở nhà số 1 với\n",
            "mở cái đèn tầng_hầm đi\n",
            "5 phút nữa mở giùm cái tivi samsung phòng khách lầu 2\n",
            "phòng hơi_lạnh tăng nhiệt_độ lên tí\n",
            "tăng nhiệt_độ máy_lạnh đi nóng quá\n",
            "tăng âm_lượng tivi\n",
            "cái tủ_lạnh tầng trệt trong bếp đang rò điện nên đóng công ổ_cắm nó lại đi\n",
            "đóng cầu giao của nhà_ở hồ_chí_minh\n",
            "giảm sáng phòng khách đi\n",
            "giảm âm_lượng tivi đi\n",
            "1 tiếng nữa tắt ổ_cắm bình nóng lạnh\n",
            "tắt điện phòng ngủ\n",
            "tắt điện phòng_ngủ hiện_tại\n",
            "tắt điện phòng ngủ lầu 1\n",
            "tắt điện phòng_ngủ lầu 2\n",
            "tắt điện phòng_ngủ lầu 3\n",
            "tắt điện phòng ngủ lầu 4\n",
            "tắt điện phòng_ngủ lầu 5\n",
            "tắt điện phòng bếp\n",
            "tắt điện phòng bếp hiện_tại\n",
            "tắt điện phòng bếp lầu 1\n",
            "tắt điện phòng bếp lầu 2\n",
            "tắt điện phòng bếp lầu 3\n",
            "tắt điện phòng bếp lầu 4\n",
            "tắt điện phòng bếp lầu 5\n",
            "tắt điện nhà_vệ_sinh\n",
            "tắt điện nhà_vệ_sinh lầu 1\n",
            "tắt điện nhà_vệ_sinh lầu 2\n",
            "tắt điện nhà_vệ_sinh lầu 3\n",
            "tắt điện nhà_vệ_sinh lầu 4\n",
            "tắt điện nhà_vệ_sinh lầu 5\n",
            "tắt điện tầng 1\n",
            "tắt điện tầng 2\n",
            "tắt điện tầng 3\n",
            "tắt điện tầng 4\n",
            "tắt điện tầng 5\n",
            "tắt điện lầu 1\n",
            "tắt điện lầu 2\n",
            "tắt điện lầu 3\n",
            "tắt điện lầu 4\n",
            "tắt điện lầu 5\n",
            "tắt điện phòng khách\n",
            "tắt tất_cả thiết_bị điện\n",
            "tắt hết điện phòng_ngủ trừ quạt ra\n",
            "lạnh quá tắt máy_lạnh đi\n",
            "lạnh quá tắt quạt đi\n",
            "nóng quá bật máy_lạnh đi\n",
            "nóng quá bật quạt đi\n",
            "chán quá bật tivi lên\n",
            "tối quá bật đèn đi\n",
            "quên tắt đèn phòng_ngủ rồi\n",
            "đừng tắt quạt treo tường tắt cái quạt_trần đi\n",
            "đừng tắt cái đèn_chùm phòng ngủ\n",
            "đừng mở ổ_cắm tivi phòng khách nha\n",
            "đừng bật quạt lạnh quá\n",
            "đừng bật máy_lạnh lạnh quá\n",
            "đừng bật đèn chói quá\n",
            "lầu 1 phòng_ngủ chưa tắt đèn\n",
            "phòng_khách lầu trệt chưa tắt tivi\n",
            "phòng_khách lầu 1 chưa tắt tivi\n",
            "phòng_khách lầu 2 chưa tắt tivi\n",
            "phòng_khách lầu 3 chưa tắt tivi\n",
            "phòng khách lầu 4 chưa tắt tivi\n",
            "phòng_khách lầu 5 chưa tắt tivi\n",
            "hãy soi_sáng căn bếp ngay hôm_nay\n",
            "rọi bếp ngày_mai\n",
            "bật_đèn trong bếp sau 10 giờ\n",
            "bật_đèn trong bếp trong 1 ngày\n",
            "hãy chiếu_sáng phòng ăn ngày hôm_nay\n",
            "đừng chiếu_sáng phòng ăn ngày hôm_nay\n",
            "chiếu_sáng phòng ăn vào ngày_mai\n",
            "bật_đèn trong phòng dinin sau 10 giờ\n",
            "bật_đèn trong phòng ăn trong 1 ngày\n",
            "bật_đèn trong phòng tắm\n",
            "đừng bật đèn ngay bây_giờ\n",
            "tôi sẽ phát_điên lên nếu bạn bật đèn ngay bây_giờ\n",
            "tôi sẽ không thích nếu bạn không bật đèn ngay bây_giờ\n",
            "đèn trong phòng khách có sáng không\n",
            "bạn vui_lòng chiếu_sáng phòng khách trong một giờ nữa giúp tôi\n",
            "thật tuyệt nếu bạn bật đèn cho tôi\n",
            "bật_đèn trong bếp\n",
            "tắt đèn trong phòng khách sau vài giờ\n",
            "tắt đèn ở tầng hầm\n",
            "tắt đèn trong phòng ăn\n",
            "tôi muốn nếu bạn làm cho nó sáng hơn\n",
            "tôi muốn nếu bạn làm cho nó sáng hơn trong 3 phút\n",
            "bạn có_thể làm cho nó sáng hơn ở đây trong 8 phút không\n",
            "bạn có_thể làm cho nó tối hơn ở đây\n",
            "đừng làm cho nó tối hơn ở đây\n",
            "tôi sẽ phát_điên lên nếu bạn bật đèn trong phòng ăn ngay bây_giờ\n",
            "đèn trong phòng tắm có sáng không\n",
            "đèn trong phòng tắm có sáng không\n",
            "đèn trong phòng tắm đã bật lâu chưa\n",
            "đèn trong bếp đã tắt lâu chưa\n",
            "đèn trong phòng tắm có sáng không\n",
            "bạn đã bật đèn lên chưa\n",
            "bạn có_thể vui_lòng bật đèn lên được không\n",
            "giúp tôi bật đèn\n",
            "bạn có_thể vui_lòng bật đèn trong một phút không\n",
            "giúp tôi bật đèn sau một phút\n",
            "ở đây tối quá\n",
            "tôi không_thể nhìn thấy gì cả\n",
            "làm sáng căn phòng\n",
            "bạn có_thể vui_lòng bật_đèn sau vài phút được không\n",
            "giúp tôi bật đèn sau vài phút\n",
            "làm sáng căn phòng trong vài phút\n",
            "làm sáng căn phòng trong 5 phút\n",
            "tắt đèn trong bếp sau 10 phút\n",
            "đừng tắt đèn trong bếp sau 10 phút\n",
            "làm cho nó bớt tối hơn trên gác mái bây_giờ\n",
            "bật_đèn trên gác_xép\n",
            "chiếu_sáng tầng áp mái\n",
            "tắt đèn sau một giờ\n",
            "bật_đèn\n",
            "nhấp_nháy đèn cho tôi\n",
            "bật tủ_lạnh\n",
            "bật lò nướng\n",
            "bật hệ_thống âm_nhạc\n",
            "tắt đèn trên gác_xép\n",
            "tắt đèn ở tầng hầm\n",
            "tắt đèn trong bếp\n",
            "tắt đèn trong thư_viện\n",
            "tắt đèn trong bếp\n",
            "tắt đèn trong phòng ăn\n",
            "tắt đèn trong hầm\n",
            "tắt đèn trên gác_xép\n",
            "tắt đèn ở tầng hầm\n",
            "tắt đèn trong bếp\n",
            "tắt đèn trong thư_viện\n",
            "tắt đèn trong bếp\n",
            "tắt đèn trong phòng ăn\n",
            "tắt đèn trong hầm\n",
            "tắt thứ gì đó sau 45 phút\n",
            "bật một cái gì đó\n",
            "đặt một thiết_bị ngẫu_nhiên trong 7 ngày\n",
            "đèn chiếu sáng\n",
            "bạn có_thể làm cho nó bớt tối trong phòng tắm vào ngày_mai\n",
            "bạn có_thể làm cho nó bớt tối trong nhà_vệ_sinh vào ngày_mai\n",
            "bật_đèn trên gác mái sau một giờ nữa\n",
            "bạn có_thể tắt đèn trong hầm được không\n",
            "tôi sẽ phát_điên lên nếu bạn bật đèn ngay bây_giờ\n",
            "trời tối trong hầm\n",
            "nó tối trong phòng tắm\n",
            "nó là tối trong phòng khách\n",
            "trong thư_viện trời tối\n",
            "đó là tối trong nhà_vệ_sinh\n",
            "trời tối trên gác_xép\n",
            "trời tối trong tầng hầm\n",
            "bạn có_thể làm cho nó bớt tối trong phòng ăn trong 5 phút không\n",
            "bạn có_thể làm cho nó bớt tối trong hầm trong 85 phút không\n",
            "bạn có_thể làm cho nó bớt tối trên gác mái trong 5 phút không\n",
            "bạn có_thể làm cho nó bớt tối trong nhà_bếp trong 45 phút không\n",
            "bạn có_thể làm cho nó bớt tối trong toilet trong 15 phút không\n",
            "bạn có_thể làm cho nó bớt tối trong thư_viện trong 50 phút không\n",
            "bạn có_thể làm cho nó bớt tối trong tầng_hầm trong 5 phút không\n",
            "bạn có_thể làm cho nó sáng hơn trong phòng ăn sau 15 giờ không\n",
            "bạn có_thể làm cho nó sáng hơn trong hầm trong 50 giờ không\n",
            "bạn có_thể làm cho nó sáng hơn trên gác mái trong một giờ nữa không\n",
            "bạn có_thể làm cho nó trở_nên tươi_sáng hơn trong nhà_bếp trong 5 ngày\n",
            "bạn có_thể làm cho nó sáng hơn trong nhà_vệ_sinh trong 5 tuần\n",
            "bạn có_thể làm cho nó sáng hơn trong thư_viện trong 2 phút không\n",
            "bạn có_thể làm cho nó sáng hơn ở tầng_hầm trong 5 phút không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trong phòng ăn sau 15 giờ không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trong hầm trong 50 giờ không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trên gác mái trong một giờ không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trong bếp trong 5 ngày không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trong nhà_vệ_sinh trong 5 tuần không\n",
            "bạn có_thể làm cho nó bớt sáng hơn trong thư_viện trong 2 phút không\n",
            "bạn có_thể làm cho nó bớt sáng hơn ở tầng_hầm trong 5 phút không\n",
            "đừng tắt đèn\n",
            "bật_đèn khi tôi về đến nhà\n",
            "bật_đèn khi bên ngoài trời tối\n",
            "tắt đèn khi bên ngoài có ánh_sáng nhẹ\n",
            "có bất_kỳ chuyển_động nào từ tháng 9 đến tháng 10 không\n",
            "có phải camera trong phòng tắm vào ngày hôm_qua không\n",
            "có phải là camera trong nhà_vệ_sinh vào ngày hôm_qua\n",
            "có chuyển_động không\n",
            "có chuyển_động với trình xem chuyển_động không\n",
            "bạn có_thể cho tôi biết nếu có bất_kỳ chuyển_động nào trên máy quay bên ngoài\n",
            "có bất_kỳ chuyển_động nào trên camera bên ngoài không\n",
            "có chuyển_động trên camera trong nhà_bếp ngày hôm_qua không\n",
            "có chuyển_động trên máy_ảnh trên gác mái ngày hôm_qua không\n",
            "có chuyển_động trên camera trong phòng tắm ngày hôm_qua không\n",
            "có chuyển_động trên camera ở sân sau ngày hôm_qua không\n",
            "có chuyển_động trên camera trong phòng khách ngày hôm_qua không\n",
            "tôi muốn rằng bạn sẽ cho tôi biết nếu camera trong phòng khách đang bật\n",
            "nơi nào có người lạ đêm nay ở nhà tôi\n",
            "bạn có thấy động_tĩnh gì tối nay không\n",
            "có điều gì đó kỳ_lạ bên ngoài\n",
            "có điều gì tôi nên biết về chuyển_động này không\n",
            "bạn nhìn thấy gì trên camera\n",
            "có bao_nhiêu máy_ảnh\n",
            "tất_cả các camera được lắp_đặt ở đâu\n",
            "có nhiều chuyển_động trên camera gần đây không\n",
            "máy_ảnh hiện đang quay là gì\n",
            "ngày_nay con_người có xuất_hiện trên camera không\n",
            "bạn có_thể cho tôi biết nếu có chuyển_động từ tháng 9 đến tháng 10 không\n",
            "máy_ảnh đã bật chưa\n",
            "mở_máy quay\n",
            "máy_ảnh ghi và xem\n",
            "bạn có_thể cho tôi một_số thông_tin về camera ở sân sau được không\n",
            "hôm_nay có chuyển_động nào trên máy quay không\n",
            "có một người bên ngoài hôm_nay\n",
            "có bất_kỳ chuyển_động nào trên máy quay ngày hôm_qua không\n",
            "có một người bên ngoài ngày hôm_qua\n",
            "có bất_kỳ chuyển_động nào trong giờ qua camera bên ngoài không\n",
            "có bất_kỳ chuyển_động nào ở sân sau ngày hôm_qua không\n",
            "hôm_nay có hoạt_động bất_thường trên camera ở sân sau không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng tắm trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi trên gác mái trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng ăn trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi ở tầng_hầm trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi trong thư_viện trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi trong bếp trong một giờ nữa không\n",
            "bạn có_thể bật hệ_thống sưởi trong nhà_vệ_sinh trong một giờ nữa không\n",
            "trong phòng tắm_hơi lạnh\n",
            "trên gác_xép hơi lạnh\n",
            "trong phòng ăn hơi_lạnh\n",
            "ở tầng hầm hơi lạnh\n",
            "trong thư_viện hơi_lạnh\n",
            "trong bếp hơi lạnh\n",
            "trong nhà_vệ_sinh hơi_lạnh\n",
            "đừng bật hệ_thống sưởi bây_giờ\n",
            "bạn có_thể giảm độ nóng ở tầng_hầm trong một giờ không\n",
            "bạn có_thể giảm độ nóng trong hầm trong một giờ không\n",
            "bạn có_thể giảm nhiệt_độ trong nhà_bếp trong một giờ không\n",
            "bạn có_thể giảm độ nóng trong nhà_vệ_sinh trong một giờ không\n",
            "bạn có_thể hạ nhiệt_độ sưởi trên gác mái trong một giờ không\n",
            "bạn có_thể giảm nhiệt_độ trong nhà_bếp trong một giờ không\n",
            "bạn có_thể giảm độ nóng trong phòng ăn trong một giờ không\n",
            "trời lạnh trong phòng tắm\n",
            "trời lạnh trên gác_xép\n",
            "trời lạnh trong phòng ăn\n",
            "trời lạnh ở tầng hầm\n",
            "trong thư_viện thật lạnh\n",
            "trời lạnh trong bếp\n",
            "nó lạnh trong nhà_vệ_sinh\n",
            "trời nóng trong phòng tắm\n",
            "trời nóng trên gác_xép\n",
            "trời nóng trong phòng ăn\n",
            "trời nóng ở tầng hầm\n",
            "nó đang nóng trong thư_viện\n",
            "nó đang nóng trong nhà_bếp\n",
            "nó nóng trong nhà_vệ_sinh\n",
            "bạn có_thể bật hệ_thống sưởi trong bếp trong 10 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong 11 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong 5 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong một phút giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong 2 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong 1 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng khách trong một phút giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong thư_viện trong 11 phút nữa giúp tôi được không\n",
            "bạn có_thể bật hệ_thống sưởi trong thư_viện trong 5 phút nữa giúp tôi được không\n",
            "tắt hệ_thống sưởi trong phòng tắm sau một giờ\n",
            "đừng tắt hệ_thống sưởi trong một giờ\n",
            "làm cho nó bớt lạnh hơn trong hầm_rượu\n",
            "làm cho nó lạnh hơn trong tầng hầm\n",
            "bạn có_thể bật hệ_thống sưởi trong phòng tắm cao hơn không\n",
            "mặt_trời chói mắt tôi trong nhà_vệ_sinh\n",
            "tắt cửa_chớp\n",
            "vui_lòng mở_cửa_chớp trong một phút cho tôi\n",
            "hãy để cửa sập ở tầng_hầm trong một giờ nữa\n",
            "đóng cửa_chớp trong nhà_bếp\n",
            "đóng cửa_chớp trong một phút\n",
            "nó quá nhẹ trong thư_viện\n",
            "hãy tắt cửa_chớp trong tầng_hầm khi bên ngoài có ánh_sáng nhẹ\n",
            "mở_cửa_chớp trong thư_viện khi bên ngoài trời sáng\n",
            "nó quá sáng trong nhà_bếp\n",
            "đừng làm bất_cứ điều gì khi trời nóng trong bếp\n",
            "bạn có_thể mở cửa nhà để xe khi tôi về nhà cho tôi được không\n",
            "hãy để cửa nhà để xe xuống ngay bây_giờ\n",
            "bạn có_thể mở cửa nhà để xe trong một giờ nữa không\n",
            "vui_lòng đóng_cửa ga_ra trong ngày cho tôi\n",
            "đóng cửa nhà để xe trong một giờ nữa cho tôi\n",
            "bạn có_thể mở cửa nhà để xe vào ngày_mai\n",
            "bạn có_thể mở cửa nhà để xe xuống ngày hôm_qua\n",
            "bạn có_thể mở cửa nhà để xe vào ngày_mai\n",
            "bạn có_thể mở cửa nhà để xe vào ngày hôm_qua\n",
            "đóng_cửa ga_ra trong một phút cho tôi\n",
            "đóng cửa nhà để xe trong một ngày cho tôi\n",
            "mở_cửa ga_ra trong một phút cho tôi\n",
            "mở_cửa ga_ra trong một ngày cho tôi\n",
            "hãy chắc_chắn rằng xe của tôi có_thể ra khỏi nhà để xe\n",
            "bạn sẽ đảm_bảo rằng xe của tôi không va vào cửa nhà để xe chứ\n",
            "đóng_cửa ga_ra trong 21 phút nữa cho tôi\n",
            "tắt cửa ga_ra trong 15 phút nữa cho tôi\n",
            "đóng_cửa ga_ra trong vòng mười bốn phút nữa cho tôi\n",
            "đóng_cửa ga_ra trong vòng mười sáu phút cho tôi\n",
            "đừng đóng cửa ga_ra trong 43 phút nữa cho tôi\n",
            "đừng đóng cửa ga_ra trong 112 phút nữa cho tôi\n",
            "đừng đóng cửa ga_ra trong vòng mười tám phút nữa đối_với tôi\n",
            "đừng đóng cửa ga_ra trong vòng mười sáu phút nữa đối_với tôi\n",
            "bạn có_thể đừng làm gì đó với cửa nhà để xe được không\n",
            "bạn có_thể đừng để cửa nhà để xe xuống được không\n",
            "đóng cửa nhà để xe khi tôi về đến nhà\n",
            "đóng điện cửa nhà để xe\n",
            "tắt cửa nhà để xe sau một phút\n",
            "tắt cửa nhà để xe sau một giờ nữa\n",
            "tắt cửa nhà để xe sau sáu ngày\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "train_texts = []\n",
        "corpus = []\n",
        "special_char = \"!?.,\"\n",
        "\n",
        "with open('./data/data.txt') as f:\n",
        "  data = f.readlines()\n",
        "\n",
        "  for sample in data:\n",
        "    if sample.strip():     \n",
        "      text = preprocess(sample)\n",
        "      print(text)\n",
        "      train_texts.append(text)\n",
        "\n",
        "  if not train_texts[-1]:\n",
        "    train_texts.pop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlu0hEX-EP6e"
      },
      "source": [
        "# Xây dựng tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tWPALBFb5l4q"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-14 06:53:16.025382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:16.025416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'trong': 1, 'bật': 2, 'điện': 3, 'phòng': 4, 'tắt': 5, 'không': 6, 'lầu': 7, 'bạn': 8, 'có_thể': 9, 'tôi': 10, 'cho': 11, 'đèn': 12, 'ở': 13, 'phút': 14, 'nó': 15, 'một': 16, 'làm': 17, 'khách': 18, 'phòng_ngủ': 19, 'bếp': 20, 'có': 21, 'giờ': 22, '1': 23, '5': 24, 'nữa': 25, 'tầng': 26, 'nhà': 27, 'sáng': 28, 'trên': 29, '2': 30, 'tối': 31, 'cửa': 32, 'mở': 33, 'nhà_vệ_sinh': 34, 'ngày': 35, 'hơn': 36, '3': 37, 'sau': 38, 'hệ_thống': 39, 'sưởi': 40, 'để': 41, 'đừng': 42, 'xe': 43, 'bớt': 44, 'nóng': 45, 'được': 46, 'lạnh': 47, '4': 48, 'camera': 49, 'chuyển_động': 50, 'trời': 51, 'ăn': 52, 'hầm': 53, 'ngủ': 54, 'quá': 55, 'tivi': 56, 'tắm': 57, 'thư_viện': 58, 'giúp': 59, 'đóng': 60, 'bật_đèn': 61, 'hôm_qua': 62, 'ga_ra': 63, 'gì': 64, 'cái': 65, 'đi': 66, 'chưa': 67, 'nếu': 68, 'bên': 69, 'ngoài': 70, 'quạt': 71, 'nào': 72, 'vào': 73, 'mái': 74, 'lên': 75, 'hồ_chí_minh': 76, 'tầng_hầm': 77, 'giảm': 78, 'bây_giờ': 79, 'gác': 80, 'nhà_bếp': 81, 'khi': 82, 'này': 83, 'máy_lạnh': 84, 'hà_nội': 85, 'huế': 86, 'ngay': 87, '10': 88, 'gác_xép': 89, 'hơi': 90, 'hơi_lạnh': 91, 'sài_gòn': 92, 'hãy': 93, 'hôm_nay': 94, 'ngày_mai': 95, 'sẽ': 96, 'vui_lòng': 97, 'bất_kỳ': 98, 'của': 99, 'nhiệt_độ': 100, 'đang': 101, 'phòng_khách': 102, 'chiếu_sáng': 103, 'đây': 104, 'đó': 105, 'về': 106, 'quay': 107, 'máy_ảnh': 108, 'đóng_cửa': 109, 'nhỉ': 110, 'hiện_tại': 111, 'với': 112, 'số': 113, 'căn': 114, 'vài': 115, 'đã': 116, 'là': 117, '15': 118, 'đến': 119, 'tháng': 120, 'biết': 121, 'sân': 122, 'độ': 123, 'cửa_chớp': 124, 'vòng': 125, 'mười': 126, 'rồi': 127, 'trệt': 128, 'xem': 129, 'và': 130, 'hết': 131, 'thiết_bị': 132, 'nay': 133, 'nên': 134, 'tất_cả': 135, 'tăng': 136, 'ổ_cắm': 137, 'phát_điên': 138, 'muốn': 139, 'thấy': 140, '50': 141, 'nhẹ': 142, 'máy': 143, 'rằng': 144, 'người': 145, 'điều': 146, 'xuống': 147, 'sáu': 148, 'đâu': 149, 'đức': 150, 'giùm': 151, 'quạt_trần': 152, 'các': 153, 'bình': 154, 'trừ': 155, 'âm_lượng': 156, 'tủ_lạnh': 157, 'ra': 158, 'chói': 159, 'thật': 160, 'lâu': 161, 'nhìn': 162, '45': 163, 'tuần': 164, 'ánh_sáng': 165, 'từ': 166, '9': 167, 'phải': 168, '11': 169, 'mở_cửa_chớp': 170, 'mở_cửa': 171, 'đối_với': 172, 'trần': 173, 'cần': 174, 'khang': 175, 'hộ': 176, 'gò_vấp': 177, 'hành_lang': 178, 'điều_hòa': 179, 'không_khí': 180, 'mát': 181, 'rèm': 182, 'samsung': 183, 'tí': 184, 'rò': 185, 'công': 186, 'lại': 187, 'cầu': 188, 'giao': 189, 'nhà_ở': 190, 'tiếng': 191, 'chán': 192, 'quên': 193, 'treo': 194, 'tường': 195, 'đèn_chùm': 196, 'nha': 197, 'soi_sáng': 198, 'rọi': 199, 'dinin': 200, 'thích': 201, 'tuyệt': 202, '8': 203, 'không_thể': 204, 'cả': 205, 'áp': 206, 'nhấp_nháy': 207, 'lò': 208, 'nướng': 209, 'âm_nhạc': 210, 'thứ': 211, 'đặt': 212, 'ngẫu_nhiên': 213, '7': 214, 'chiếu': 215, '85': 216, 'toilet': 217, 'trở_nên': 218, 'tươi_sáng': 219, 'trình': 220, 'nơi': 221, 'lạ': 222, 'đêm': 223, 'động_tĩnh': 224, 'kỳ_lạ': 225, 'bao_nhiêu': 226, 'lắp_đặt': 227, 'nhiều': 228, 'gần': 229, 'hiện': 230, 'ngày_nay': 231, 'con_người': 232, 'xuất_hiện': 233, 'mở_máy': 234, 'ghi': 235, 'một_số': 236, 'thông_tin': 237, 'qua': 238, 'hoạt_động': 239, 'bất_thường': 240, 'tắm_hơi': 241, 'hạ': 242, 'hầm_rượu': 243, 'cao': 244, 'mặt_trời': 245, 'mắt': 246, 'sập': 247, 'bất_cứ': 248, 'chắc_chắn': 249, 'khỏi': 250, 'đảm_bảo': 251, 'va': 252, 'chứ': 253, '21': 254, 'bốn': 255, '43': 256, '112': 257, 'tám': 258}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n'\n",
        "    # oov_token='<OOV>'\n",
        ")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFgQ6eFEcv1"
      },
      "source": [
        "# Tách câu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1xaY_jkZEWFT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 83],\n",
              " [1, 83, 90],\n",
              " [1, 83, 90, 31],\n",
              " [1, 83, 90, 31, 110],\n",
              " [1, 83, 90, 31, 110, 2],\n",
              " [1, 83, 90, 31, 110, 2, 12],\n",
              " [1, 83, 90, 31, 110, 2, 12, 173],\n",
              " [1, 83, 90, 31, 110, 2, 12, 173, 75],\n",
              " [1, 83],\n",
              " [1, 83, 28]]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_sequences = []\n",
        "for line in train_texts:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "input_sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F96h7jeWEeI7"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(word_index) + 1\n",
        "MAX_LEN = max([len(sent) for sent in input_sequences])\n",
        "BATCH_SIZE = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A0uH1gLpEhka"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trong này\n",
            "trong này hơi\n",
            "trong này hơi tối\n",
            "trong này hơi tối nhỉ\n",
            "trong này hơi tối nhỉ bật\n",
            "trong này hơi tối nhỉ bật đèn\n",
            "trong này hơi tối nhỉ bật đèn trần\n",
            "trong này hơi tối nhỉ bật đèn trần lên\n",
            "trong này\n",
            "trong này sáng\n"
          ]
        }
      ],
      "source": [
        "for point in input_sequences[:10]:\n",
        "  print(\" \".join(tokenizer.sequences_to_texts([point])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl_8Q9j7Elft"
      },
      "source": [
        "# Chia features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DT1RDX0bEkZS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "padded_input_sequences = np.array(pad_sequences(input_sequences, maxlen = MAX_LEN, padding=\"pre\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SwiJ4FwGEr2T"
      },
      "outputs": [],
      "source": [
        "predictors, label = padded_input_sequences[:,:-1], padded_input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uNsBmmWiEw1L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trong --> này\n",
            "trong này --> hơi\n",
            "trong này hơi --> tối\n",
            "trong này hơi tối --> nhỉ\n",
            "trong này hơi tối nhỉ --> bật\n",
            "trong này hơi tối nhỉ bật --> đèn\n",
            "trong này hơi tối nhỉ bật đèn --> trần\n",
            "trong này hơi tối nhỉ bật đèn trần --> lên\n",
            "trong --> này\n",
            "trong này --> sáng\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  text = \" \".join(tokenizer.sequences_to_texts(predictors[i:i+1]))\n",
        "  label_decode = tokenizer.sequences_to_texts([label[i:i+1]])[0]\n",
        "  print(f\"{text} --> {label_decode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jTT1YdIZEx4Y"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "label = to_categorical(label, num_classes = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B94FhXniE3KL"
      },
      "source": [
        "# Xây dựng model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jkRlM5CGE1TI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-14 06:53:19.384621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-06-14 06:53:19.385032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-06-14 06:53:19.385476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.385522: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.385560: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.387638: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.387682: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.387717: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
            "2022-06-14 06:53:19.387725: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-06-14 06:53:19.388394: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 15, 128)           33152     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 15, 128)          512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 15, 1024)         2625536   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 15, 1024)         6295552   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 512)              2623488   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 259)               66563     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,777,155\n",
            "Trainable params: 11,776,387\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, Flatten, Embedding, BatchNormalization, LSTM, Dense, Dropout\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = VOCAB_SIZE, output_dim = 128, input_length=MAX_LEN - 1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(256)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sJL0cOWiE7FH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "\n",
        "dt = datetime.now()\n",
        "dt_s = '_'.join(str(dt).split(' '))\n",
        "print(dt_s)\n",
        "\n",
        "checkpoint_path = f'./model/generate_text/test_lstm_{dt_s}.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    verbose=1,\n",
        "    monitor='loss',\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        "    save_best_only=True),\n",
        "\n",
        "    keras.callbacks.EarlyStopping(\n",
        "    verbose=1,\n",
        "    monitor='loss',\n",
        "    patience=25)\n",
        "]\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "# tf.data.experimental.enable_debug_mode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCkzNFyJE-vx",
        "outputId": "8bbe9620-8a23-4d73-e7cc-31df4f247e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 4.9868 - accuracy: 0.1331\n",
            "Epoch 1: loss improved from inf to 4.98682, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 36s 369ms/step - loss: 4.9868 - accuracy: 0.1331\n",
            "Epoch 2/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 4.1997 - accuracy: 0.2333\n",
            "Epoch 2: loss improved from 4.98682 to 4.19973, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 4.1997 - accuracy: 0.2333\n",
            "Epoch 3/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 3.6797 - accuracy: 0.3079\n",
            "Epoch 3: loss improved from 4.19973 to 3.67966, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 3.6797 - accuracy: 0.3079\n",
            "Epoch 4/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 3.3273 - accuracy: 0.3387\n",
            "Epoch 4: loss improved from 3.67966 to 3.32730, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 3.3273 - accuracy: 0.3387\n",
            "Epoch 5/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 3.0501 - accuracy: 0.3809\n",
            "Epoch 5: loss improved from 3.32730 to 3.05011, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 3.0501 - accuracy: 0.3809\n",
            "Epoch 6/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.8113 - accuracy: 0.4260\n",
            "Epoch 6: loss improved from 3.05011 to 2.81134, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 2.8113 - accuracy: 0.4260\n",
            "Epoch 7/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.6338 - accuracy: 0.4479\n",
            "Epoch 7: loss improved from 2.81134 to 2.63380, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 382ms/step - loss: 2.6338 - accuracy: 0.4479\n",
            "Epoch 8/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.4760 - accuracy: 0.4665\n",
            "Epoch 8: loss improved from 2.63380 to 2.47605, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 2.4760 - accuracy: 0.4665\n",
            "Epoch 9/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.3155 - accuracy: 0.4970\n",
            "Epoch 9: loss improved from 2.47605 to 2.31550, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 386ms/step - loss: 2.3155 - accuracy: 0.4970\n",
            "Epoch 10/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.2181 - accuracy: 0.5039\n",
            "Epoch 10: loss improved from 2.31550 to 2.21811, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 382ms/step - loss: 2.2181 - accuracy: 0.5039\n",
            "Epoch 11/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.0950 - accuracy: 0.5193\n",
            "Epoch 11: loss improved from 2.21811 to 2.09500, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 2.0950 - accuracy: 0.5193\n",
            "Epoch 12/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 2.0011 - accuracy: 0.5521\n",
            "Epoch 12: loss improved from 2.09500 to 2.00113, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 2.0011 - accuracy: 0.5521\n",
            "Epoch 13/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.8993 - accuracy: 0.5525\n",
            "Epoch 13: loss improved from 2.00113 to 1.89930, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 1.8993 - accuracy: 0.5525\n",
            "Epoch 14/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.8416 - accuracy: 0.5712\n",
            "Epoch 14: loss improved from 1.89930 to 1.84161, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 1.8416 - accuracy: 0.5712\n",
            "Epoch 15/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.7727 - accuracy: 0.5659\n",
            "Epoch 15: loss improved from 1.84161 to 1.77268, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 379ms/step - loss: 1.7727 - accuracy: 0.5659\n",
            "Epoch 16/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.7119 - accuracy: 0.5890\n",
            "Epoch 16: loss improved from 1.77268 to 1.71186, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 1.7119 - accuracy: 0.5890\n",
            "Epoch 17/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.6623 - accuracy: 0.5886\n",
            "Epoch 17: loss improved from 1.71186 to 1.66227, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 386ms/step - loss: 1.6623 - accuracy: 0.5886\n",
            "Epoch 18/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.5857 - accuracy: 0.6105\n",
            "Epoch 18: loss improved from 1.66227 to 1.58568, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 380ms/step - loss: 1.5857 - accuracy: 0.6105\n",
            "Epoch 19/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.5511 - accuracy: 0.6081\n",
            "Epoch 19: loss improved from 1.58568 to 1.55109, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 380ms/step - loss: 1.5511 - accuracy: 0.6081\n",
            "Epoch 20/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.5039 - accuracy: 0.6260\n",
            "Epoch 20: loss improved from 1.55109 to 1.50385, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 379ms/step - loss: 1.5039 - accuracy: 0.6260\n",
            "Epoch 21/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.4621 - accuracy: 0.6288\n",
            "Epoch 21: loss improved from 1.50385 to 1.46206, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 384ms/step - loss: 1.4621 - accuracy: 0.6288\n",
            "Epoch 22/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.4094 - accuracy: 0.6438\n",
            "Epoch 22: loss improved from 1.46206 to 1.40939, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 384ms/step - loss: 1.4094 - accuracy: 0.6438\n",
            "Epoch 23/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.3704 - accuracy: 0.6430\n",
            "Epoch 23: loss improved from 1.40939 to 1.37041, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 387ms/step - loss: 1.3704 - accuracy: 0.6430\n",
            "Epoch 24/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.3279 - accuracy: 0.6548\n",
            "Epoch 24: loss improved from 1.37041 to 1.32792, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 377ms/step - loss: 1.3279 - accuracy: 0.6548\n",
            "Epoch 25/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.3200 - accuracy: 0.6600\n",
            "Epoch 25: loss improved from 1.32792 to 1.32004, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 1.3200 - accuracy: 0.6600\n",
            "Epoch 26/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.2742 - accuracy: 0.6665\n",
            "Epoch 26: loss improved from 1.32004 to 1.27425, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 381ms/step - loss: 1.2742 - accuracy: 0.6665\n",
            "Epoch 27/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.2806 - accuracy: 0.6592\n",
            "Epoch 27: loss did not improve from 1.27425\n",
            "78/78 [==============================] - 29s 380ms/step - loss: 1.2806 - accuracy: 0.6592\n",
            "Epoch 28/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.2331 - accuracy: 0.6726\n",
            "Epoch 28: loss improved from 1.27425 to 1.23307, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 384ms/step - loss: 1.2331 - accuracy: 0.6726\n",
            "Epoch 29/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.2145 - accuracy: 0.6649\n",
            "Epoch 29: loss improved from 1.23307 to 1.21445, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 381ms/step - loss: 1.2145 - accuracy: 0.6649\n",
            "Epoch 30/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.1673 - accuracy: 0.6775\n",
            "Epoch 30: loss improved from 1.21445 to 1.16726, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 389ms/step - loss: 1.1673 - accuracy: 0.6775\n",
            "Epoch 31/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.1291 - accuracy: 0.6925\n",
            "Epoch 31: loss improved from 1.16726 to 1.12912, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 389ms/step - loss: 1.1291 - accuracy: 0.6925\n",
            "Epoch 32/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.1320 - accuracy: 0.6864\n",
            "Epoch 32: loss did not improve from 1.12912\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 1.1320 - accuracy: 0.6864\n",
            "Epoch 33/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.1150 - accuracy: 0.6905\n",
            "Epoch 33: loss improved from 1.12912 to 1.11505, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 1.1150 - accuracy: 0.6905\n",
            "Epoch 34/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0943 - accuracy: 0.6925\n",
            "Epoch 34: loss improved from 1.11505 to 1.09432, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 381ms/step - loss: 1.0943 - accuracy: 0.6925\n",
            "Epoch 35/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0822 - accuracy: 0.7043\n",
            "Epoch 35: loss improved from 1.09432 to 1.08222, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 390ms/step - loss: 1.0822 - accuracy: 0.7043\n",
            "Epoch 36/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0507 - accuracy: 0.7047\n",
            "Epoch 36: loss improved from 1.08222 to 1.05069, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 380ms/step - loss: 1.0507 - accuracy: 0.7047\n",
            "Epoch 37/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0288 - accuracy: 0.7063\n",
            "Epoch 37: loss improved from 1.05069 to 1.02881, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 1.0288 - accuracy: 0.7063\n",
            "Epoch 38/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0283 - accuracy: 0.7002\n",
            "Epoch 38: loss improved from 1.02881 to 1.02826, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 388ms/step - loss: 1.0283 - accuracy: 0.7002\n",
            "Epoch 39/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.7160\n",
            "Epoch 39: loss improved from 1.02826 to 1.00393, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 384ms/step - loss: 1.0039 - accuracy: 0.7160\n",
            "Epoch 40/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9964 - accuracy: 0.7148\n",
            "Epoch 40: loss improved from 1.00393 to 0.99643, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 0.9964 - accuracy: 0.7148\n",
            "Epoch 41/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9784 - accuracy: 0.7229\n",
            "Epoch 41: loss improved from 0.99643 to 0.97843, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 377ms/step - loss: 0.9784 - accuracy: 0.7229\n",
            "Epoch 42/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9521 - accuracy: 0.7310\n",
            "Epoch 42: loss improved from 0.97843 to 0.95209, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 379ms/step - loss: 0.9521 - accuracy: 0.7310\n",
            "Epoch 43/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9479 - accuracy: 0.7213\n",
            "Epoch 43: loss improved from 0.95209 to 0.94785, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 31s 406ms/step - loss: 0.9479 - accuracy: 0.7213\n",
            "Epoch 44/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9394 - accuracy: 0.7266\n",
            "Epoch 44: loss improved from 0.94785 to 0.93937, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 39s 499ms/step - loss: 0.9394 - accuracy: 0.7266\n",
            "Epoch 45/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9383 - accuracy: 0.7233\n",
            "Epoch 45: loss improved from 0.93937 to 0.93829, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 487ms/step - loss: 0.9383 - accuracy: 0.7233\n",
            "Epoch 46/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9157 - accuracy: 0.7318\n",
            "Epoch 46: loss improved from 0.93829 to 0.91567, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 481ms/step - loss: 0.9157 - accuracy: 0.7318\n",
            "Epoch 47/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.9122 - accuracy: 0.7290\n",
            "Epoch 47: loss improved from 0.91567 to 0.91218, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 492ms/step - loss: 0.9122 - accuracy: 0.7290\n",
            "Epoch 48/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8799 - accuracy: 0.7387\n",
            "Epoch 48: loss improved from 0.91218 to 0.87990, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 493ms/step - loss: 0.8799 - accuracy: 0.7387\n",
            "Epoch 49/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8935 - accuracy: 0.7347\n",
            "Epoch 49: loss did not improve from 0.87990\n",
            "78/78 [==============================] - 37s 482ms/step - loss: 0.8935 - accuracy: 0.7347\n",
            "Epoch 50/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.7379\n",
            "Epoch 50: loss improved from 0.87990 to 0.86982, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 37s 476ms/step - loss: 0.8698 - accuracy: 0.7379\n",
            "Epoch 51/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.7489\n",
            "Epoch 51: loss improved from 0.86982 to 0.85613, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 35s 444ms/step - loss: 0.8561 - accuracy: 0.7489\n",
            "Epoch 52/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8515 - accuracy: 0.7408\n",
            "Epoch 52: loss improved from 0.85613 to 0.85153, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 0.8515 - accuracy: 0.7408\n",
            "Epoch 53/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8312 - accuracy: 0.7497\n",
            "Epoch 53: loss improved from 0.85153 to 0.83120, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 384ms/step - loss: 0.8312 - accuracy: 0.7497\n",
            "Epoch 54/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8337 - accuracy: 0.7432\n",
            "Epoch 54: loss did not improve from 0.83120\n",
            "78/78 [==============================] - 30s 381ms/step - loss: 0.8337 - accuracy: 0.7432\n",
            "Epoch 55/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8354 - accuracy: 0.7408\n",
            "Epoch 55: loss did not improve from 0.83120\n",
            "78/78 [==============================] - 31s 391ms/step - loss: 0.8354 - accuracy: 0.7408\n",
            "Epoch 56/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8213 - accuracy: 0.7550\n",
            "Epoch 56: loss improved from 0.83120 to 0.82129, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 46s 592ms/step - loss: 0.8213 - accuracy: 0.7550\n",
            "Epoch 57/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8139 - accuracy: 0.7473\n",
            "Epoch 57: loss improved from 0.82129 to 0.81387, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 51s 665ms/step - loss: 0.8139 - accuracy: 0.7473\n",
            "Epoch 58/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.7562\n",
            "Epoch 58: loss improved from 0.81387 to 0.78771, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 53s 675ms/step - loss: 0.7877 - accuracy: 0.7562\n",
            "Epoch 59/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.8092 - accuracy: 0.7493\n",
            "Epoch 59: loss did not improve from 0.78771\n",
            "78/78 [==============================] - 51s 651ms/step - loss: 0.8092 - accuracy: 0.7493\n",
            "Epoch 60/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7854 - accuracy: 0.7623\n",
            "Epoch 60: loss improved from 0.78771 to 0.78543, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 51s 654ms/step - loss: 0.7854 - accuracy: 0.7623\n",
            "Epoch 61/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7915 - accuracy: 0.7485\n",
            "Epoch 61: loss did not improve from 0.78543\n",
            "78/78 [==============================] - 51s 660ms/step - loss: 0.7915 - accuracy: 0.7485\n",
            "Epoch 62/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7852 - accuracy: 0.7574\n",
            "Epoch 62: loss improved from 0.78543 to 0.78525, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 51s 661ms/step - loss: 0.7852 - accuracy: 0.7574\n",
            "Epoch 63/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7682 - accuracy: 0.7643\n",
            "Epoch 63: loss improved from 0.78525 to 0.76816, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 52s 662ms/step - loss: 0.7682 - accuracy: 0.7643\n",
            "Epoch 64/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7716 - accuracy: 0.7533\n",
            "Epoch 64: loss did not improve from 0.76816\n",
            "78/78 [==============================] - 33s 419ms/step - loss: 0.7716 - accuracy: 0.7533\n",
            "Epoch 65/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7627 - accuracy: 0.7574\n",
            "Epoch 65: loss improved from 0.76816 to 0.76273, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 0.7627 - accuracy: 0.7574\n",
            "Epoch 66/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.7546\n",
            "Epoch 66: loss did not improve from 0.76273\n",
            "78/78 [==============================] - 32s 401ms/step - loss: 0.7669 - accuracy: 0.7546\n",
            "Epoch 67/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7549 - accuracy: 0.7602\n",
            "Epoch 67: loss improved from 0.76273 to 0.75485, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 488ms/step - loss: 0.7549 - accuracy: 0.7602\n",
            "Epoch 68/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7395 - accuracy: 0.7574\n",
            "Epoch 68: loss improved from 0.75485 to 0.73950, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 488ms/step - loss: 0.7395 - accuracy: 0.7574\n",
            "Epoch 69/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7439 - accuracy: 0.7615\n",
            "Epoch 69: loss did not improve from 0.73950\n",
            "78/78 [==============================] - 37s 481ms/step - loss: 0.7439 - accuracy: 0.7615\n",
            "Epoch 70/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7514 - accuracy: 0.7647\n",
            "Epoch 70: loss did not improve from 0.73950\n",
            "78/78 [==============================] - 37s 470ms/step - loss: 0.7514 - accuracy: 0.7647\n",
            "Epoch 71/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7278 - accuracy: 0.7627\n",
            "Epoch 71: loss improved from 0.73950 to 0.72780, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 491ms/step - loss: 0.7278 - accuracy: 0.7627\n",
            "Epoch 72/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7450 - accuracy: 0.7566\n",
            "Epoch 72: loss did not improve from 0.72780\n",
            "78/78 [==============================] - 37s 471ms/step - loss: 0.7450 - accuracy: 0.7566\n",
            "Epoch 73/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.7529\n",
            "Epoch 73: loss did not improve from 0.72780\n",
            "78/78 [==============================] - 37s 481ms/step - loss: 0.7340 - accuracy: 0.7529\n",
            "Epoch 74/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7137 - accuracy: 0.7631\n",
            "Epoch 74: loss improved from 0.72780 to 0.71369, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 39s 497ms/step - loss: 0.7137 - accuracy: 0.7631\n",
            "Epoch 75/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7144 - accuracy: 0.7611\n",
            "Epoch 75: loss did not improve from 0.71369\n",
            "78/78 [==============================] - 38s 482ms/step - loss: 0.7144 - accuracy: 0.7611\n",
            "Epoch 76/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.7582\n",
            "Epoch 76: loss did not improve from 0.71369\n",
            "78/78 [==============================] - 38s 481ms/step - loss: 0.7187 - accuracy: 0.7582\n",
            "Epoch 77/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7132 - accuracy: 0.7619\n",
            "Epoch 77: loss improved from 0.71369 to 0.71317, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 37s 482ms/step - loss: 0.7132 - accuracy: 0.7619\n",
            "Epoch 78/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7225 - accuracy: 0.7655\n",
            "Epoch 78: loss did not improve from 0.71317\n",
            "78/78 [==============================] - 38s 488ms/step - loss: 0.7225 - accuracy: 0.7655\n",
            "Epoch 79/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7164 - accuracy: 0.7558\n",
            "Epoch 79: loss did not improve from 0.71317\n",
            "78/78 [==============================] - 37s 472ms/step - loss: 0.7164 - accuracy: 0.7558\n",
            "Epoch 80/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7061 - accuracy: 0.7586\n",
            "Epoch 80: loss improved from 0.71317 to 0.70609, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 484ms/step - loss: 0.7061 - accuracy: 0.7586\n",
            "Epoch 81/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.7024 - accuracy: 0.7651\n",
            "Epoch 81: loss improved from 0.70609 to 0.70236, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 490ms/step - loss: 0.7024 - accuracy: 0.7651\n",
            "Epoch 82/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6889 - accuracy: 0.7712\n",
            "Epoch 82: loss improved from 0.70236 to 0.68885, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 39s 502ms/step - loss: 0.6889 - accuracy: 0.7712\n",
            "Epoch 83/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.7582\n",
            "Epoch 83: loss did not improve from 0.68885\n",
            "78/78 [==============================] - 37s 480ms/step - loss: 0.6970 - accuracy: 0.7582\n",
            "Epoch 84/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.7643\n",
            "Epoch 84: loss improved from 0.68885 to 0.68677, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 38s 481ms/step - loss: 0.6868 - accuracy: 0.7643\n",
            "Epoch 85/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.7598\n",
            "Epoch 85: loss did not improve from 0.68677\n",
            "78/78 [==============================] - 37s 477ms/step - loss: 0.6943 - accuracy: 0.7598\n",
            "Epoch 86/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.7627\n",
            "Epoch 86: loss improved from 0.68677 to 0.68304, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 39s 496ms/step - loss: 0.6830 - accuracy: 0.7627\n",
            "Epoch 87/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.7663\n",
            "Epoch 87: loss improved from 0.68304 to 0.67152, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 36s 465ms/step - loss: 0.6715 - accuracy: 0.7663\n",
            "Epoch 88/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6835 - accuracy: 0.7663\n",
            "Epoch 88: loss did not improve from 0.67152\n",
            "78/78 [==============================] - 31s 394ms/step - loss: 0.6835 - accuracy: 0.7663\n",
            "Epoch 89/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.7696\n",
            "Epoch 89: loss did not improve from 0.67152\n",
            "78/78 [==============================] - 41s 526ms/step - loss: 0.6768 - accuracy: 0.7696\n",
            "Epoch 90/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6718 - accuracy: 0.7692\n",
            "Epoch 90: loss did not improve from 0.67152\n",
            "78/78 [==============================] - 49s 633ms/step - loss: 0.6718 - accuracy: 0.7692\n",
            "Epoch 91/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6583 - accuracy: 0.7655\n",
            "Epoch 91: loss improved from 0.67152 to 0.65830, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 50s 640ms/step - loss: 0.6583 - accuracy: 0.7655\n",
            "Epoch 92/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6681 - accuracy: 0.7680\n",
            "Epoch 92: loss did not improve from 0.65830\n",
            "78/78 [==============================] - 51s 652ms/step - loss: 0.6681 - accuracy: 0.7680\n",
            "Epoch 93/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.7631\n",
            "Epoch 93: loss did not improve from 0.65830\n",
            "78/78 [==============================] - 50s 645ms/step - loss: 0.6676 - accuracy: 0.7631\n",
            "Epoch 94/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6761 - accuracy: 0.7639\n",
            "Epoch 94: loss did not improve from 0.65830\n",
            "78/78 [==============================] - 49s 620ms/step - loss: 0.6761 - accuracy: 0.7639\n",
            "Epoch 95/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.7663\n",
            "Epoch 95: loss did not improve from 0.65830\n",
            "78/78 [==============================] - 50s 637ms/step - loss: 0.6608 - accuracy: 0.7663\n",
            "Epoch 96/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.7590\n",
            "Epoch 96: loss improved from 0.65830 to 0.65302, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 50s 635ms/step - loss: 0.6530 - accuracy: 0.7590\n",
            "Epoch 97/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6607 - accuracy: 0.7655\n",
            "Epoch 97: loss did not improve from 0.65302\n",
            "78/78 [==============================] - 49s 624ms/step - loss: 0.6607 - accuracy: 0.7655\n",
            "Epoch 98/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6577 - accuracy: 0.7712\n",
            "Epoch 98: loss did not improve from 0.65302\n",
            "78/78 [==============================] - 54s 685ms/step - loss: 0.6577 - accuracy: 0.7712\n",
            "Epoch 99/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6574 - accuracy: 0.7708\n",
            "Epoch 99: loss did not improve from 0.65302\n",
            "78/78 [==============================] - 51s 648ms/step - loss: 0.6574 - accuracy: 0.7708\n",
            "Epoch 100/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.7647\n",
            "Epoch 100: loss did not improve from 0.65302\n",
            "78/78 [==============================] - 49s 630ms/step - loss: 0.6590 - accuracy: 0.7647\n",
            "Epoch 101/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6475 - accuracy: 0.7700\n",
            "Epoch 101: loss improved from 0.65302 to 0.64754, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 50s 653ms/step - loss: 0.6475 - accuracy: 0.7700\n",
            "Epoch 102/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6544 - accuracy: 0.7639\n",
            "Epoch 102: loss did not improve from 0.64754\n",
            "78/78 [==============================] - 49s 632ms/step - loss: 0.6544 - accuracy: 0.7639\n",
            "Epoch 103/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.7643\n",
            "Epoch 103: loss did not improve from 0.64754\n",
            "78/78 [==============================] - 49s 625ms/step - loss: 0.6495 - accuracy: 0.7643\n",
            "Epoch 104/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6536 - accuracy: 0.7651\n",
            "Epoch 104: loss did not improve from 0.64754\n",
            "78/78 [==============================] - 50s 644ms/step - loss: 0.6536 - accuracy: 0.7651\n",
            "Epoch 105/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.7684\n",
            "Epoch 105: loss improved from 0.64754 to 0.63749, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 52s 664ms/step - loss: 0.6375 - accuracy: 0.7684\n",
            "Epoch 106/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6374 - accuracy: 0.7720\n",
            "Epoch 106: loss improved from 0.63749 to 0.63738, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 50s 641ms/step - loss: 0.6374 - accuracy: 0.7720\n",
            "Epoch 107/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6445 - accuracy: 0.7659\n",
            "Epoch 107: loss did not improve from 0.63738\n",
            "78/78 [==============================] - 49s 622ms/step - loss: 0.6445 - accuracy: 0.7659\n",
            "Epoch 108/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6405 - accuracy: 0.7700\n",
            "Epoch 108: loss did not improve from 0.63738\n",
            "78/78 [==============================] - 48s 617ms/step - loss: 0.6405 - accuracy: 0.7700\n",
            "Epoch 109/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6352 - accuracy: 0.7647\n",
            "Epoch 109: loss improved from 0.63738 to 0.63519, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 49s 628ms/step - loss: 0.6352 - accuracy: 0.7647\n",
            "Epoch 110/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6240 - accuracy: 0.7785\n",
            "Epoch 110: loss improved from 0.63519 to 0.62404, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 45s 573ms/step - loss: 0.6240 - accuracy: 0.7785\n",
            "Epoch 111/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.7692\n",
            "Epoch 111: loss did not improve from 0.62404\n",
            "78/78 [==============================] - 31s 400ms/step - loss: 0.6461 - accuracy: 0.7692\n",
            "Epoch 112/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.7680\n",
            "Epoch 112: loss did not improve from 0.62404\n",
            "78/78 [==============================] - 42s 535ms/step - loss: 0.6395 - accuracy: 0.7680\n",
            "Epoch 113/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.7700\n",
            "Epoch 113: loss did not improve from 0.62404\n",
            "78/78 [==============================] - 41s 525ms/step - loss: 0.6313 - accuracy: 0.7700\n",
            "Epoch 114/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6220 - accuracy: 0.7720\n",
            "Epoch 114: loss improved from 0.62404 to 0.62197, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 42s 541ms/step - loss: 0.6220 - accuracy: 0.7720\n",
            "Epoch 115/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.7688\n",
            "Epoch 115: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 533ms/step - loss: 0.6325 - accuracy: 0.7688\n",
            "Epoch 116/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.7659\n",
            "Epoch 116: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 539ms/step - loss: 0.6314 - accuracy: 0.7659\n",
            "Epoch 117/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.7692\n",
            "Epoch 117: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 542ms/step - loss: 0.6302 - accuracy: 0.7692\n",
            "Epoch 118/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.7655\n",
            "Epoch 118: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 544ms/step - loss: 0.6358 - accuracy: 0.7655\n",
            "Epoch 119/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6347 - accuracy: 0.7692\n",
            "Epoch 119: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 535ms/step - loss: 0.6347 - accuracy: 0.7692\n",
            "Epoch 120/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.7716\n",
            "Epoch 120: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 534ms/step - loss: 0.6283 - accuracy: 0.7716\n",
            "Epoch 121/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.7680\n",
            "Epoch 121: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 42s 537ms/step - loss: 0.6322 - accuracy: 0.7680\n",
            "Epoch 122/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6230 - accuracy: 0.7700\n",
            "Epoch 122: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 43s 554ms/step - loss: 0.6230 - accuracy: 0.7700\n",
            "Epoch 123/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.7659\n",
            "Epoch 123: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 44s 559ms/step - loss: 0.6250 - accuracy: 0.7659\n",
            "Epoch 124/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7716\n",
            "Epoch 124: loss did not improve from 0.62197\n",
            "78/78 [==============================] - 44s 566ms/step - loss: 0.6281 - accuracy: 0.7716\n",
            "Epoch 125/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.7724\n",
            "Epoch 125: loss improved from 0.62197 to 0.60894, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 46s 583ms/step - loss: 0.6089 - accuracy: 0.7724\n",
            "Epoch 126/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6086 - accuracy: 0.7748\n",
            "Epoch 126: loss improved from 0.60894 to 0.60862, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 43s 548ms/step - loss: 0.6086 - accuracy: 0.7748\n",
            "Epoch 127/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6147 - accuracy: 0.7748\n",
            "Epoch 127: loss did not improve from 0.60862\n",
            "78/78 [==============================] - 41s 523ms/step - loss: 0.6147 - accuracy: 0.7748\n",
            "Epoch 128/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6207 - accuracy: 0.7692\n",
            "Epoch 128: loss did not improve from 0.60862\n",
            "78/78 [==============================] - 43s 553ms/step - loss: 0.6207 - accuracy: 0.7692\n",
            "Epoch 129/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6192 - accuracy: 0.7817\n",
            "Epoch 129: loss did not improve from 0.60862\n",
            "78/78 [==============================] - 40s 505ms/step - loss: 0.6192 - accuracy: 0.7817\n",
            "Epoch 130/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.7732\n",
            "Epoch 130: loss did not improve from 0.60862\n",
            "78/78 [==============================] - 39s 506ms/step - loss: 0.6102 - accuracy: 0.7732\n",
            "Epoch 131/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5975 - accuracy: 0.7822\n",
            "Epoch 131: loss improved from 0.60862 to 0.59746, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 40s 517ms/step - loss: 0.5975 - accuracy: 0.7822\n",
            "Epoch 132/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.7785\n",
            "Epoch 132: loss did not improve from 0.59746\n",
            "78/78 [==============================] - 40s 513ms/step - loss: 0.6011 - accuracy: 0.7785\n",
            "Epoch 133/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6137 - accuracy: 0.7675\n",
            "Epoch 133: loss did not improve from 0.59746\n",
            "78/78 [==============================] - 33s 421ms/step - loss: 0.6137 - accuracy: 0.7675\n",
            "Epoch 134/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6069 - accuracy: 0.7724\n",
            "Epoch 134: loss did not improve from 0.59746\n",
            "78/78 [==============================] - 31s 395ms/step - loss: 0.6069 - accuracy: 0.7724\n",
            "Epoch 135/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6066 - accuracy: 0.7708\n",
            "Epoch 135: loss did not improve from 0.59746\n",
            "78/78 [==============================] - 30s 385ms/step - loss: 0.6066 - accuracy: 0.7708\n",
            "Epoch 136/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.7708\n",
            "Epoch 136: loss did not improve from 0.59746\n",
            "78/78 [==============================] - 29s 379ms/step - loss: 0.6157 - accuracy: 0.7708\n",
            "Epoch 137/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5922 - accuracy: 0.7797\n",
            "Epoch 137: loss improved from 0.59746 to 0.59224, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 0.5922 - accuracy: 0.7797\n",
            "Epoch 138/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.7724\n",
            "Epoch 138: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 382ms/step - loss: 0.5984 - accuracy: 0.7724\n",
            "Epoch 139/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.7684\n",
            "Epoch 139: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 31s 399ms/step - loss: 0.6085 - accuracy: 0.7684\n",
            "Epoch 140/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.7724\n",
            "Epoch 140: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 32s 402ms/step - loss: 0.6101 - accuracy: 0.7724\n",
            "Epoch 141/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6114 - accuracy: 0.7667\n",
            "Epoch 141: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 31s 396ms/step - loss: 0.6114 - accuracy: 0.7667\n",
            "Epoch 142/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5995 - accuracy: 0.7692\n",
            "Epoch 142: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 375ms/step - loss: 0.5995 - accuracy: 0.7692\n",
            "Epoch 143/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.7716\n",
            "Epoch 143: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 0.6116 - accuracy: 0.7716\n",
            "Epoch 144/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5971 - accuracy: 0.7826\n",
            "Epoch 144: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 0.5971 - accuracy: 0.7826\n",
            "Epoch 145/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7732\n",
            "Epoch 145: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 31s 392ms/step - loss: 0.5981 - accuracy: 0.7732\n",
            "Epoch 146/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6023 - accuracy: 0.7708\n",
            "Epoch 146: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 386ms/step - loss: 0.6023 - accuracy: 0.7708\n",
            "Epoch 147/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.7716\n",
            "Epoch 147: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 29s 371ms/step - loss: 0.6011 - accuracy: 0.7716\n",
            "Epoch 148/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5928 - accuracy: 0.7708\n",
            "Epoch 148: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 391ms/step - loss: 0.5928 - accuracy: 0.7708\n",
            "Epoch 149/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5969 - accuracy: 0.7777\n",
            "Epoch 149: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 388ms/step - loss: 0.5969 - accuracy: 0.7777\n",
            "Epoch 150/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.7659\n",
            "Epoch 150: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 388ms/step - loss: 0.6012 - accuracy: 0.7659\n",
            "Epoch 151/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6060 - accuracy: 0.7777\n",
            "Epoch 151: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 31s 399ms/step - loss: 0.6060 - accuracy: 0.7777\n",
            "Epoch 152/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.7781\n",
            "Epoch 152: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 392ms/step - loss: 0.5937 - accuracy: 0.7781\n",
            "Epoch 153/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.7700\n",
            "Epoch 153: loss did not improve from 0.59224\n",
            "78/78 [==============================] - 30s 390ms/step - loss: 0.5984 - accuracy: 0.7700\n",
            "Epoch 154/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.7789\n",
            "Epoch 154: loss improved from 0.59224 to 0.59050, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 387ms/step - loss: 0.5905 - accuracy: 0.7789\n",
            "Epoch 155/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.7785\n",
            "Epoch 155: loss did not improve from 0.59050\n",
            "78/78 [==============================] - 30s 385ms/step - loss: 0.5981 - accuracy: 0.7785\n",
            "Epoch 156/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5834 - accuracy: 0.7732\n",
            "Epoch 156: loss improved from 0.59050 to 0.58335, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 31s 392ms/step - loss: 0.5834 - accuracy: 0.7732\n",
            "Epoch 157/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5987 - accuracy: 0.7680\n",
            "Epoch 157: loss did not improve from 0.58335\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 0.5987 - accuracy: 0.7680\n",
            "Epoch 158/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5960 - accuracy: 0.7684\n",
            "Epoch 158: loss did not improve from 0.58335\n",
            "78/78 [==============================] - 29s 367ms/step - loss: 0.5960 - accuracy: 0.7684\n",
            "Epoch 159/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5930 - accuracy: 0.7753\n",
            "Epoch 159: loss did not improve from 0.58335\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 0.5930 - accuracy: 0.7753\n",
            "Epoch 160/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5873 - accuracy: 0.7850\n",
            "Epoch 160: loss did not improve from 0.58335\n",
            "78/78 [==============================] - 30s 388ms/step - loss: 0.5873 - accuracy: 0.7850\n",
            "Epoch 161/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5890 - accuracy: 0.7732\n",
            "Epoch 161: loss did not improve from 0.58335\n",
            "78/78 [==============================] - 30s 391ms/step - loss: 0.5890 - accuracy: 0.7732\n",
            "Epoch 162/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.7757\n",
            "Epoch 162: loss improved from 0.58335 to 0.58314, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 0.5831 - accuracy: 0.7757\n",
            "Epoch 163/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.7781\n",
            "Epoch 163: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 29s 372ms/step - loss: 0.5933 - accuracy: 0.7781\n",
            "Epoch 164/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5857 - accuracy: 0.7773\n",
            "Epoch 164: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 0.5857 - accuracy: 0.7773\n",
            "Epoch 165/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5898 - accuracy: 0.7667\n",
            "Epoch 165: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 30s 377ms/step - loss: 0.5898 - accuracy: 0.7667\n",
            "Epoch 166/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.7692\n",
            "Epoch 166: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 0.5835 - accuracy: 0.7692\n",
            "Epoch 167/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.7744\n",
            "Epoch 167: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 0.5838 - accuracy: 0.7744\n",
            "Epoch 168/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.7680\n",
            "Epoch 168: loss did not improve from 0.58314\n",
            "78/78 [==============================] - 29s 376ms/step - loss: 0.5905 - accuracy: 0.7680\n",
            "Epoch 169/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5807 - accuracy: 0.7740\n",
            "Epoch 169: loss improved from 0.58314 to 0.58073, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 0.5807 - accuracy: 0.7740\n",
            "Epoch 170/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7684\n",
            "Epoch 170: loss did not improve from 0.58073\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 0.5980 - accuracy: 0.7684\n",
            "Epoch 171/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.7785\n",
            "Epoch 171: loss did not improve from 0.58073\n",
            "78/78 [==============================] - 29s 369ms/step - loss: 0.5822 - accuracy: 0.7785\n",
            "Epoch 172/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.7720\n",
            "Epoch 172: loss did not improve from 0.58073\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 0.5808 - accuracy: 0.7720\n",
            "Epoch 173/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.7785\n",
            "Epoch 173: loss did not improve from 0.58073\n",
            "78/78 [==============================] - 29s 373ms/step - loss: 0.5945 - accuracy: 0.7785\n",
            "Epoch 174/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.6007 - accuracy: 0.7708\n",
            "Epoch 174: loss did not improve from 0.58073\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 0.6007 - accuracy: 0.7708\n",
            "Epoch 175/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5779 - accuracy: 0.7789\n",
            "Epoch 175: loss improved from 0.58073 to 0.57785, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 386ms/step - loss: 0.5779 - accuracy: 0.7789\n",
            "Epoch 176/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5808 - accuracy: 0.7688\n",
            "Epoch 176: loss did not improve from 0.57785\n",
            "78/78 [==============================] - 30s 394ms/step - loss: 0.5808 - accuracy: 0.7688\n",
            "Epoch 177/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.7708\n",
            "Epoch 177: loss did not improve from 0.57785\n",
            "78/78 [==============================] - 38s 490ms/step - loss: 0.5888 - accuracy: 0.7708\n",
            "Epoch 178/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5783 - accuracy: 0.7728\n",
            "Epoch 178: loss did not improve from 0.57785\n",
            "78/78 [==============================] - 46s 590ms/step - loss: 0.5783 - accuracy: 0.7728\n",
            "Epoch 179/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.7773\n",
            "Epoch 179: loss improved from 0.57785 to 0.57553, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 44s 560ms/step - loss: 0.5755 - accuracy: 0.7773\n",
            "Epoch 180/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.7716\n",
            "Epoch 180: loss did not improve from 0.57553\n",
            "78/78 [==============================] - 44s 566ms/step - loss: 0.5862 - accuracy: 0.7716\n",
            "Epoch 181/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.7675\n",
            "Epoch 181: loss did not improve from 0.57553\n",
            "78/78 [==============================] - 44s 559ms/step - loss: 0.5835 - accuracy: 0.7675\n",
            "Epoch 182/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5866 - accuracy: 0.7748\n",
            "Epoch 182: loss did not improve from 0.57553\n",
            "78/78 [==============================] - 45s 579ms/step - loss: 0.5866 - accuracy: 0.7748\n",
            "Epoch 183/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5760 - accuracy: 0.7781\n",
            "Epoch 183: loss did not improve from 0.57553\n",
            "78/78 [==============================] - 46s 583ms/step - loss: 0.5760 - accuracy: 0.7781\n",
            "Epoch 184/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.7744\n",
            "Epoch 184: loss did not improve from 0.57553\n",
            "78/78 [==============================] - 48s 614ms/step - loss: 0.5776 - accuracy: 0.7744\n",
            "Epoch 185/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7761\n",
            "Epoch 185: loss improved from 0.57553 to 0.57522, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 46s 583ms/step - loss: 0.5752 - accuracy: 0.7761\n",
            "Epoch 186/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.7712\n",
            "Epoch 186: loss improved from 0.57522 to 0.56896, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 49s 624ms/step - loss: 0.5690 - accuracy: 0.7712\n",
            "Epoch 187/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.7728\n",
            "Epoch 187: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 48s 614ms/step - loss: 0.5802 - accuracy: 0.7728\n",
            "Epoch 188/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5752 - accuracy: 0.7781\n",
            "Epoch 188: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 47s 599ms/step - loss: 0.5752 - accuracy: 0.7781\n",
            "Epoch 189/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.7785\n",
            "Epoch 189: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 51s 654ms/step - loss: 0.5696 - accuracy: 0.7785\n",
            "Epoch 190/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5798 - accuracy: 0.7692\n",
            "Epoch 190: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 58s 744ms/step - loss: 0.5798 - accuracy: 0.7692\n",
            "Epoch 191/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7781\n",
            "Epoch 191: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 55s 707ms/step - loss: 0.5702 - accuracy: 0.7781\n",
            "Epoch 192/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7724\n",
            "Epoch 192: loss did not improve from 0.56896\n",
            "78/78 [==============================] - 48s 610ms/step - loss: 0.5810 - accuracy: 0.7724\n",
            "Epoch 193/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.7765\n",
            "Epoch 193: loss improved from 0.56896 to 0.56403, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 48s 616ms/step - loss: 0.5640 - accuracy: 0.7765\n",
            "Epoch 194/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7769\n",
            "Epoch 194: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 46s 585ms/step - loss: 0.5705 - accuracy: 0.7769\n",
            "Epoch 195/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.7757\n",
            "Epoch 195: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 48s 610ms/step - loss: 0.5728 - accuracy: 0.7757\n",
            "Epoch 196/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.7736\n",
            "Epoch 196: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 54s 694ms/step - loss: 0.5731 - accuracy: 0.7736\n",
            "Epoch 197/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.7728\n",
            "Epoch 197: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 55s 700ms/step - loss: 0.5810 - accuracy: 0.7728\n",
            "Epoch 198/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.7740\n",
            "Epoch 198: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 50s 642ms/step - loss: 0.5713 - accuracy: 0.7740\n",
            "Epoch 199/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5751 - accuracy: 0.7748\n",
            "Epoch 199: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 47s 597ms/step - loss: 0.5751 - accuracy: 0.7748\n",
            "Epoch 200/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5653 - accuracy: 0.7817\n",
            "Epoch 200: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 48s 607ms/step - loss: 0.5653 - accuracy: 0.7817\n",
            "Epoch 201/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5671 - accuracy: 0.7716\n",
            "Epoch 201: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 33s 421ms/step - loss: 0.5671 - accuracy: 0.7716\n",
            "Epoch 202/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5695 - accuracy: 0.7748\n",
            "Epoch 202: loss did not improve from 0.56403\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 0.5695 - accuracy: 0.7748\n",
            "Epoch 203/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.7761\n",
            "Epoch 203: loss improved from 0.56403 to 0.56240, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 0.5624 - accuracy: 0.7761\n",
            "Epoch 204/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5802 - accuracy: 0.7712\n",
            "Epoch 204: loss did not improve from 0.56240\n",
            "78/78 [==============================] - 29s 379ms/step - loss: 0.5802 - accuracy: 0.7712\n",
            "Epoch 205/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.7740\n",
            "Epoch 205: loss did not improve from 0.56240\n",
            "78/78 [==============================] - 30s 391ms/step - loss: 0.5629 - accuracy: 0.7740\n",
            "Epoch 206/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7712\n",
            "Epoch 206: loss did not improve from 0.56240\n",
            "78/78 [==============================] - 29s 377ms/step - loss: 0.5702 - accuracy: 0.7712\n",
            "Epoch 207/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7809\n",
            "Epoch 207: loss improved from 0.56240 to 0.56074, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 29s 378ms/step - loss: 0.5607 - accuracy: 0.7809\n",
            "Epoch 208/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.7716\n",
            "Epoch 208: loss did not improve from 0.56074\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 0.5696 - accuracy: 0.7716\n",
            "Epoch 209/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5763 - accuracy: 0.7744\n",
            "Epoch 209: loss did not improve from 0.56074\n",
            "78/78 [==============================] - 30s 378ms/step - loss: 0.5763 - accuracy: 0.7744\n",
            "Epoch 210/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.7761\n",
            "Epoch 210: loss improved from 0.56074 to 0.56030, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 0.5603 - accuracy: 0.7761\n",
            "Epoch 211/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5702 - accuracy: 0.7732\n",
            "Epoch 211: loss did not improve from 0.56030\n",
            "78/78 [==============================] - 29s 374ms/step - loss: 0.5702 - accuracy: 0.7732\n",
            "Epoch 212/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5819 - accuracy: 0.7659\n",
            "Epoch 212: loss did not improve from 0.56030\n",
            "78/78 [==============================] - 30s 382ms/step - loss: 0.5819 - accuracy: 0.7659\n",
            "Epoch 213/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5674 - accuracy: 0.7740\n",
            "Epoch 213: loss did not improve from 0.56030\n",
            "78/78 [==============================] - 29s 375ms/step - loss: 0.5674 - accuracy: 0.7740\n",
            "Epoch 214/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.7748\n",
            "Epoch 214: loss did not improve from 0.56030\n",
            "78/78 [==============================] - 30s 379ms/step - loss: 0.5710 - accuracy: 0.7748\n",
            "Epoch 215/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5533 - accuracy: 0.7736\n",
            "Epoch 215: loss improved from 0.56030 to 0.55328, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 30s 383ms/step - loss: 0.5533 - accuracy: 0.7736\n",
            "Epoch 216/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5675 - accuracy: 0.7740\n",
            "Epoch 216: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 32s 413ms/step - loss: 0.5675 - accuracy: 0.7740\n",
            "Epoch 217/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5654 - accuracy: 0.7700\n",
            "Epoch 217: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 47s 598ms/step - loss: 0.5654 - accuracy: 0.7700\n",
            "Epoch 218/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5726 - accuracy: 0.7769\n",
            "Epoch 218: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 48s 611ms/step - loss: 0.5726 - accuracy: 0.7769\n",
            "Epoch 219/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5562 - accuracy: 0.7842\n",
            "Epoch 219: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 48s 612ms/step - loss: 0.5562 - accuracy: 0.7842\n",
            "Epoch 220/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5666 - accuracy: 0.7716\n",
            "Epoch 220: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 49s 625ms/step - loss: 0.5666 - accuracy: 0.7716\n",
            "Epoch 221/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7753\n",
            "Epoch 221: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 49s 624ms/step - loss: 0.5610 - accuracy: 0.7753\n",
            "Epoch 222/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5551 - accuracy: 0.7777\n",
            "Epoch 222: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 49s 632ms/step - loss: 0.5551 - accuracy: 0.7777\n",
            "Epoch 223/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7748\n",
            "Epoch 223: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 31s 401ms/step - loss: 0.5650 - accuracy: 0.7748\n",
            "Epoch 224/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.7789\n",
            "Epoch 224: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 30s 391ms/step - loss: 0.5697 - accuracy: 0.7789\n",
            "Epoch 225/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7769\n",
            "Epoch 225: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 30s 388ms/step - loss: 0.5621 - accuracy: 0.7769\n",
            "Epoch 226/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5601 - accuracy: 0.7744\n",
            "Epoch 226: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 30s 382ms/step - loss: 0.5601 - accuracy: 0.7744\n",
            "Epoch 227/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5584 - accuracy: 0.7748\n",
            "Epoch 227: loss did not improve from 0.55328\n",
            "78/78 [==============================] - 33s 421ms/step - loss: 0.5584 - accuracy: 0.7748\n",
            "Epoch 228/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.7728\n",
            "Epoch 228: loss improved from 0.55328 to 0.55010, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 33s 418ms/step - loss: 0.5501 - accuracy: 0.7728\n",
            "Epoch 229/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.7773\n",
            "Epoch 229: loss did not improve from 0.55010\n",
            "78/78 [==============================] - 32s 407ms/step - loss: 0.5578 - accuracy: 0.7773\n",
            "Epoch 230/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.7773\n",
            "Epoch 230: loss did not improve from 0.55010\n",
            "78/78 [==============================] - 39s 500ms/step - loss: 0.5668 - accuracy: 0.7773\n",
            "Epoch 231/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.7805\n",
            "Epoch 231: loss improved from 0.55010 to 0.54538, saving model to ./model/generate_text/test_lstm.ckpt\n",
            "78/78 [==============================] - 34s 437ms/step - loss: 0.5454 - accuracy: 0.7805\n",
            "Epoch 232/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5640 - accuracy: 0.7732\n",
            "Epoch 232: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 30s 389ms/step - loss: 0.5640 - accuracy: 0.7732\n",
            "Epoch 233/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.7761\n",
            "Epoch 233: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 37s 481ms/step - loss: 0.5536 - accuracy: 0.7761\n",
            "Epoch 234/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5616 - accuracy: 0.7744\n",
            "Epoch 234: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 629ms/step - loss: 0.5616 - accuracy: 0.7744\n",
            "Epoch 235/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5482 - accuracy: 0.7838\n",
            "Epoch 235: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 626ms/step - loss: 0.5482 - accuracy: 0.7838\n",
            "Epoch 236/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7765\n",
            "Epoch 236: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 616ms/step - loss: 0.5589 - accuracy: 0.7765\n",
            "Epoch 237/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5569 - accuracy: 0.7773\n",
            "Epoch 237: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 628ms/step - loss: 0.5569 - accuracy: 0.7773\n",
            "Epoch 238/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.7700\n",
            "Epoch 238: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 606ms/step - loss: 0.5568 - accuracy: 0.7700\n",
            "Epoch 239/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.7712\n",
            "Epoch 239: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 624ms/step - loss: 0.5634 - accuracy: 0.7712\n",
            "Epoch 240/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7817\n",
            "Epoch 240: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 624ms/step - loss: 0.5528 - accuracy: 0.7817\n",
            "Epoch 241/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.7708\n",
            "Epoch 241: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 627ms/step - loss: 0.5582 - accuracy: 0.7708\n",
            "Epoch 242/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.7769\n",
            "Epoch 242: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 619ms/step - loss: 0.5508 - accuracy: 0.7769\n",
            "Epoch 243/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.7728\n",
            "Epoch 243: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 617ms/step - loss: 0.5622 - accuracy: 0.7728\n",
            "Epoch 244/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5705 - accuracy: 0.7757\n",
            "Epoch 244: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 629ms/step - loss: 0.5705 - accuracy: 0.7757\n",
            "Epoch 245/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5484 - accuracy: 0.7838\n",
            "Epoch 245: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 50s 643ms/step - loss: 0.5484 - accuracy: 0.7838\n",
            "Epoch 246/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7744\n",
            "Epoch 246: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 49s 628ms/step - loss: 0.5528 - accuracy: 0.7744\n",
            "Epoch 247/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.7696\n",
            "Epoch 247: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 619ms/step - loss: 0.5614 - accuracy: 0.7696\n",
            "Epoch 248/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5543 - accuracy: 0.7720\n",
            "Epoch 248: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 50s 642ms/step - loss: 0.5543 - accuracy: 0.7720\n",
            "Epoch 249/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5501 - accuracy: 0.7712\n",
            "Epoch 249: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 39s 496ms/step - loss: 0.5501 - accuracy: 0.7712\n",
            "Epoch 250/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.7789\n",
            "Epoch 250: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 33s 415ms/step - loss: 0.5512 - accuracy: 0.7789\n",
            "Epoch 251/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5509 - accuracy: 0.7793\n",
            "Epoch 251: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 48s 619ms/step - loss: 0.5509 - accuracy: 0.7793\n",
            "Epoch 252/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7842\n",
            "Epoch 252: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 47s 603ms/step - loss: 0.5528 - accuracy: 0.7842\n",
            "Epoch 253/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5540 - accuracy: 0.7817\n",
            "Epoch 253: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 47s 598ms/step - loss: 0.5540 - accuracy: 0.7817\n",
            "Epoch 254/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5563 - accuracy: 0.7753\n",
            "Epoch 254: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 47s 597ms/step - loss: 0.5563 - accuracy: 0.7753\n",
            "Epoch 255/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5466 - accuracy: 0.7809\n",
            "Epoch 255: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 45s 580ms/step - loss: 0.5466 - accuracy: 0.7809\n",
            "Epoch 256/1000\n",
            "78/78 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7757\n",
            "Epoch 256: loss did not improve from 0.54538\n",
            "78/78 [==============================] - 47s 606ms/step - loss: 0.5541 - accuracy: 0.7757\n",
            "Epoch 256: early stopping\n"
          ]
        }
      ],
      "source": [
        " history = model.fit(predictors, label, epochs=1000, verbose=1, callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrW7lqdkFF5-"
      },
      "source": [
        "# Save and Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nU020TOSFEwy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('./model/generate_text/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "with open('./model/generate_text/input_sequences.pkl', 'wb') as f:\n",
        "    pickle.dump(padded_input_sequences, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jUK3HzQbFIJY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f07d0186e20>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "with open('./model/generate_text/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "    \n",
        "with open('./model/generate_text/input_sequences.pkl', 'rb') as f:\n",
        "    padded_input_sequences = pickle.load(f)\n",
        "\n",
        "model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7mzQE-FLsQ"
      },
      "source": [
        "# Thử tạo text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E1HVJV7MFSPv"
      },
      "outputs": [],
      "source": [
        "def gen_text(text, next_words):\n",
        "  text = preprocess(text)\n",
        "  for _ in range(next_words):\n",
        "    # Chuyển câu thành vector\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    \n",
        "    # Padding câu\n",
        "    token_list = pad_sequences([token_list], maxlen=MAX_LEN-1, padding='pre')\n",
        "    \n",
        "    # Dự đoán từ tiếp theo\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    \n",
        "    output_word = \"\"\n",
        "\n",
        "    predicted_id = np.argmax(predicted)\n",
        "    text_ls = text.split(\" \")\n",
        "\n",
        "    if predicted_id in tokenizer.index_word:\n",
        "      output_word = tokenizer.index_word[predicted_id]\n",
        "      if output_word != text_ls[-1]:\n",
        "        text_ls.append(output_word)\n",
        "    else:\n",
        "      break\n",
        "    res = \" \".join(text_ls)\n",
        "\n",
        "  return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GF6uarnVFKJQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bật tivi xem\n",
            "bật tivi xem có\n",
            "bật tivi xem có gì\n",
            "bật tivi xem có gì nào\n",
            "bật tivi xem có gì nào và\n",
            "bật tivi xem có gì nào và tắt\n",
            "bật tivi xem có gì nào và tắt hộ\n",
            "bật tivi xem có gì nào và tắt hộ cái\n",
            "bật tivi xem có gì nào và tắt hộ cái điện\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1 không\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1 không nữa\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1 không nữa giúp\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1 không nữa giúp tôi\n",
            "bật tivi xem có gì nào và tắt hộ cái điện trong bếp trong bếp không 1 không nữa giúp tôi được\n",
            "tắt tivi xem\n",
            "tắt tivi xem có\n",
            "tắt tivi xem có gì\n",
            "tắt tivi xem có gì nào\n",
            "tắt tivi xem có gì nào và\n",
            "tắt tivi xem có gì nào và tắt\n",
            "tắt tivi xem có gì nào và tắt hộ\n",
            "tắt tivi xem có gì nào và tắt hộ cái\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổ_cắm\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổcắm ổ_cắm\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổcắm ổcắm ổ_cắm\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổcắm ổcắm ổcắm ổ_cắm\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổcắm ổcắm ổcắm ổcắm ổ_cắm\n",
            "tắt tivi xem có gì nào và tắt hộ cái điện trong bếp không 1 không ổcắm ổcắm ổcắm ổcắm ổcắm ổ_cắm\n",
            "nóng quá bật\n",
            "nóng quá bật quạt\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "nóng quá bật quạt đi\n",
            "tắt đèn trong\n",
            "tắt đèn trong bếp\n",
            "tắt đèn trong bếp sau\n",
            "tắt đèn trong bếp sau 10\n",
            "tắt đèn trong bếp sau 10 phút\n",
            "tắt đèn trong bếp sau 10 phút nữa\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máylạnh máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máylạnh máylạnh máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máylạnh máylạnh máylạnh máylạnh máy_lạnh\n",
            "tắt đèn trong bếp sau 10 phút nữa giúp tôi được không cần bật máylạnh máylạnh máylạnh máylạnh máylạnh máylạnh máylạnh máy_lạnh\n",
            "tắt quạt phòng ngủ lầu\n",
            "tắt quạt phòng_ngủ lầu 5\n",
            "tắt quạt phòngngủ lầu 5 phút\n",
            "tắt quạt phòngngủ lầu 5 phút giùm\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa tắt\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa tắt tivi\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa tắt tivi\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa tắt tivi\n",
            "tắt quạt phòngngủ lầu 5 phút giùm cái tivi đi samsung phòng khách lầu 2 lầu 1 chưa tắt tivi\n",
            "đèn lầu trệt trong\n",
            "đèn lầu trệt trong phòng\n",
            "đèn lầu trệt trong phòng tắm\n",
            "đèn lầu trệt trong phòng tắm đã\n",
            "đèn lầu trệt trong phòng tắm đã bật\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n",
            "đèn lầu trệt trong phòng tắm đã bật lâu chưa\n"
          ]
        }
      ],
      "source": [
        "test_seq = [\"bật tivi\", \"tắt tivi\", \"nóng quá\", \"tắt đèn\", \"tắt quạt phòng ngủ\", \"đèn lầu trệt\"]\n",
        "new_seq = set()\n",
        "for sent in test_seq:\n",
        "  for i in range(20):\n",
        "    sent = gen_text(sent, 5 + i)\n",
        "    new_seq.add(sent)\n",
        "    print(sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "test_3.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "21c9c07779ae7f0e455ccbd002dd08b92f37501b8b1b5e4101ea11e06de5b7d2"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('ducnd')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
